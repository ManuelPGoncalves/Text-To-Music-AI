{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Game Soundtrack Generator (Text-to-Music AI) 🤖🎵🎮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mido\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration, pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 1: Extract Features from MIDI Files\n",
    "### 🎯 Goal: Convert MIDI files into structured features for machine learning.\n",
    "\n",
    "✅ Steps:\n",
    "- 1️⃣ Read MIDI files from df_labelled using mido.\n",
    "    - df_unlabelled has a very high degree of missing data.\n",
    "- 2️⃣ Extract relevant musical features, such as:\n",
    "\n",
    "    - Tempo (BPM)   \n",
    "    - Note density (number of notes per second)\n",
    "    - Chord structure & key signature\n",
    "    - Pitch variation & range\n",
    "\n",
    "- 3️⃣ Store the extracted features in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df's\n",
    "df_labelled = pd.read_csv('vgmidi_labelled.csv')\n",
    "df_unlabelled = pd.read_csv('vgmidi_unlabelled.csv')\n",
    "\n",
    "#fix path\n",
    "df_labelled['midi'] = df_labelled['midi'].str.replace('labelled/phrases/', 'labelled/midi/')\n",
    "df_unlabelled['midi'] = df_unlabelled['midi'].str.replace('data_clean/midi/', 'unlabelled/midi/')\n",
    "\n",
    "#fix naming\n",
    "df_labelled['midi'] = df_labelled['midi'].str.replace('_0', '')\n",
    "df_unlabelled['midi'] = df_unlabelled['midi'].str.replace('_0', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_features(midi_path):\n",
    "    \n",
    "    try:\n",
    "        mid = mido.MidiFile(midi_path)\n",
    "        tempos, notes = [], []\n",
    "        for track in mid.tracks:\n",
    "            for msg in track:\n",
    "                if msg.type == \"set_tempo\":\n",
    "                    tempos.append(mido.tempo2bpm(msg.tempo))\n",
    "                elif msg.type == \"note_on\":\n",
    "                    notes.append(msg.note)\n",
    "        \n",
    "        return {\n",
    "            \"avg_bpm\": np.mean(tempos) if tempos else 120,  \n",
    "            \"note_density\": len(notes) / mid.length if mid.length > 0 else len(notes),\n",
    "            \"pitch_variance\": np.var(notes) if notes else 0\n",
    "        }\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply feature extraction\n",
    "df_labelled[\"features\"] = df_labelled[\"midi\"].apply(extract_midi_features)\n",
    "\n",
    "#not for this df due to high degree of missing data\n",
    "#df_unlabelled[\"features\"] = df_unlabelled[\"midi\"].apply(extract_midi_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 2: Label df_unlabelled Using Zero-Shot Classification\n",
    "### 🎯 Goal: Since df_unlabelled lacks extracted MIDI features, we use BART zero-shot classification to predict valence/arousal labels from metadata.\n",
    "\n",
    "✅ Steps:\n",
    "\n",
    "- Use facebook/bart-large-mnli to classify game moods based on available metadata (e.g., song title, game name).\n",
    "- Assign valence & arousal values using text-based heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_midi_metadata(title):\n",
    "    labels = [\"Calm\", \"Intense\", \"Mysterious\", \"Action\"]\n",
    "    result = classifier(title, candidate_labels=labels)\n",
    "    \n",
    "    # Map moods to valence/arousal\n",
    "    mood_map = {\n",
    "        \"Calm\": (-1, -1),\n",
    "        \"Intense\": (1, -1),\n",
    "        \"Mysterious\": (-1, 1),\n",
    "        \"Action\": (1, 1)\n",
    "    }\n",
    "    return mood_map[result[\"labels\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled = df_unlabelled.dropna(subset=[\"piece\"])\n",
    "df_unlabelled[[\"valence\", \"arousal\"]] = df_unlabelled[\"piece\"].apply(classify_midi_metadata).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valence  arousal\n",
       " 1       -1         1476\n",
       "          1         1080\n",
       "-1        1          811\n",
       "         -1          482\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabelled[['valence', 'arousal']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 3: Train a Hybrid Model to Predict Valence/Arousal\n",
    "### 🎯 Goal: Train a model using MIDI features (from df_labelled) + text-based classification (from df_unlabelled).\n",
    "\n",
    "✅ Steps:\n",
    "\n",
    "- Train an ML model only on df_labelled MIDI features.\n",
    "- Use a text-based classifier to label df_unlabelled.\n",
    "- Combine both predictions for music generation.\n",
    "- Train only on df_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.json_normalize(df_labelled[\"features\"])  \n",
    "y_valence = df_labelled[\"valence\"]\n",
    "y_arousal = df_labelled[\"arousal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_valence = RandomForestClassifier(n_estimators=100).fit(X, y_valence)\n",
    "model_arousal = RandomForestClassifier(n_estimators=100).fit(X, y_arousal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 4: Generate AI Music Based on Hybrid Model Predictions\n",
    "### 🎯 Goal: Use hybrid model (ML + zero-shot classification) to generate game soundtracks using facebook/musicgen.\n",
    "\n",
    "✅ Steps:\n",
    "\n",
    "- Generate mood-based prompts using ML model (df_labelled) + zero-shot classification (df_unlabelled).\n",
    "- Feed the prompt into MusicGen for soundtrack generation.\n",
    "- MusicGen using Hybrid Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.encodec.modeling_encodec.EncodecModel'> is overwritten by shared audio_encoder config: EncodecConfig {\n",
      "  \"architectures\": [\n",
      "    \"EncodecModel\"\n",
      "  ],\n",
      "  \"audio_channels\": 1,\n",
      "  \"chunk_length_s\": null,\n",
      "  \"codebook_dim\": 128,\n",
      "  \"codebook_size\": 2048,\n",
      "  \"compress\": 2,\n",
      "  \"dilation_growth_rate\": 2,\n",
      "  \"hidden_size\": 128,\n",
      "  \"kernel_size\": 7,\n",
      "  \"last_kernel_size\": 7,\n",
      "  \"model_type\": \"encodec\",\n",
      "  \"norm_type\": \"weight_norm\",\n",
      "  \"normalize\": false,\n",
      "  \"num_filters\": 64,\n",
      "  \"num_lstm_layers\": 2,\n",
      "  \"num_residual_layers\": 1,\n",
      "  \"overlap\": null,\n",
      "  \"pad_mode\": \"reflect\",\n",
      "  \"residual_kernel_size\": 3,\n",
      "  \"sampling_rate\": 32000,\n",
      "  \"target_bandwidths\": [\n",
      "    2.2\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"trim_right_ratio\": 1.0,\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    5,\n",
      "    4,\n",
      "    4\n",
      "  ],\n",
      "  \"use_causal_conv\": false,\n",
      "  \"use_conv_shortcut\": false\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM'> is overwritten by shared decoder config: MusicgenDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"audio_channels\": 1,\n",
      "  \"bos_token_id\": 2048,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"musicgen_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 2048,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2048\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load Facebook MusicGen model\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(mood):\n",
    "    prompt = [f\"{mood} video game soundtrack\"]\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    # Generate music\n",
    "    audio_values = model.generate(**inputs)\n",
    "\n",
    "    # Ensure output is 2D (1 channel, N samples)\n",
    "    audio_values = audio_values.squeeze(0).unsqueeze(0)  \n",
    "\n",
    "    # Save the generated audio\n",
    "    torchaudio.save(f\"generated_{mood}.wav\", audio_values, sample_rate=32000)\n",
    "\n",
    "    return f\"generated_{mood}.wav\"\n",
    "\n",
    "def generate_music_from_metadata(title):\n",
    "    valence, arousal = classify_midi_metadata(title)  # Zero-shot classification\n",
    "    mood = \"Calm\" if valence == -1 and arousal == -1 else \\\n",
    "           \"Intense\" if valence == 1 and arousal == -1 else \\\n",
    "           \"Mysterious\" if valence == -1 and arousal == 1 else \"Action\"\n",
    "    \n",
    "    return generate_music(mood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_music_from_metadata(\"Dark haunted castle theme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
